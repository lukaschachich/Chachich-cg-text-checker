# Computer Generated Text Checker

A Streamlit web app that predicts whether an input sentence/review was written by a Human or generated by AI. It reproduces the feature extraction used during training (text stats, sentiment, and POS features) to ensure the model receives the exact same inputs at inference time.

## What It Does
  - Label: `Human` or `AI`
  - Confidence score and class probabilities

## Repo Structure
Key files and folders:
  - `feature_names.json`: Names and order of expected features
  - `model_metadata.json`: Model info
  - `review_classifier.pkl`: Trained XGBoost model (you need to add this)

## Prerequisites

## Setup
1. Create and activate a virtual environment (skip if you already have one):

```powershell
python -m venv venv
& "$(Resolve-Path .\venv\Scripts\Activate.ps1)"
```

2. Install dependencies:

```powershell
pip install -r requirements.txt
```

3. Download the spaCy English model (required by the app):

```powershell
python -m spacy download en_core_web_sm
```

Note: The app will auto-download the NLTK VADER lexicon if missing.

## Model Files
The app expects the trained model at:

```
model/review_classifier.pkl
```


Current code expects:

```python
model_dir = Path(__file__).resolve().parents[1] / "model"
model_path = model_dir / "review_classifier.pkl"
```

## Running the App
From the repository root:

```powershell
& "$(Resolve-Path .\venv\Scripts\Activate.ps1)"
streamlit run webapp/app.py
```

Then open the UI in your browser (Streamlit will print the local URL).

## Using the App

## Troubleshooting
  - Verify `model/review_classifier.pkl` exists.
  - Confirm the path in `get_xgb_model()` matches your layout.
  - Run `python -m spacy download en_core_web_sm` again.
  - Ensure `src/` is present; the app appends it to `sys.path`.
  - Make sure `feature_names.json` matches the training pipeline; the app aligns columns in `prepare_features_for_prediction()`.

## Training (Optional)
If you need to train or tune the model:

## License
This project is for educational use. Adjust and extend as needed for your workflow.

# Computer-Generated Text Detector

This project provides a web app and scripts to detect whether a given sentence or review was written by a human or generated by AI. While the example data is based on Amazon reviews, the core focus is on distinguishing computer-generated (CG) text from original (human-written) text using machine learning and linguistic features.

## Features
- **Input any sentence or review** to check if it is likely AI-generated or human-written.
- **Feature extraction**: Text statistics (length, word count, punctuation, average word length, uppercase ratio), part-of-speech (POS) counts, and sentiment features.
- **XGBoost classifier**: Trained to distinguish CG from human text.
- **Web app**: User-friendly Streamlit interface for predictions and feature analysis.
- **Training and tuning scripts**: For building and improving your own models.

## Project Structure

- `webapp/app.py` — Streamlit web app for inference and feature analysis
- `src/` — Preprocessing and feature extraction utilities
  - `preprocess.py` — Text cleaning
  - `feature_extraction.py` — Feature engineering
- `model/` — Model artifacts
  - `review_classifier.pkl` — Trained XGBoost model
  - `feature_names.json` — List/order of features expected by the model
  - `model_metadata.json` — Model info and metrics
- `requirements.txt` — Python dependencies
- `train_model.py` — Script to train a new model
- `tune_model.py` — Script for hyperparameter tuning and feature selection
- `eda_starter.py` — Exploratory data analysis (EDA) on datasets
- `prepare_dataset.py` — Dataset preparation/cleaning
- `*.csv` — Example datasets (see below)

## Datasets

- `cg_sentance_dataset.csv`, `fake-reviews.csv`, `processed-dataset.csv`, `processed-dataset-orig.csv` — Example datasets for training and testing. These contain both human and computer-generated text samples, with labels (`OR` for original/human, `CG` for computer-generated).

## Setup

1. **Python 3.10+ recommended**
2. Create and activate a virtual environment:
  ```powershell
  python -m venv venv
  & "$(Resolve-Path .\venv\Scripts\Activate.ps1)"
  ```
3. Install dependencies:
  ```powershell
  pip install -r requirements.txt
  ```
4. Download the spaCy English model (required):
  ```powershell
  python -m spacy download en_core_web_sm
  ```
  The app will auto-download the NLTK VADER lexicon if missing.

## Running the Web App

From the repository root:

```powershell
& "$(Resolve-Path .\venv\Scripts\Activate.ps1)"
streamlit run webapp/app.py
```

Open the local URL provided by Streamlit in your browser.

## Using the App
- Enter or paste any sentence or review.
- (Optional) Select a product category (for Amazon-style data; can be ignored for general text).
- Click "Analyze Review" to get the prediction and confidence score.
- Expand "Feature Analysis" to see extracted features.

## Model Files

The app expects the trained model at:

```
model/review_classifier.pkl
```

If you train a new model, save it here. Ensure `feature_names.json` matches the features your model expects.

## Training and Tuning

- Use `train_model.py` to train a new XGBoost classifier on your dataset.
- Use `tune_model.py` for hyperparameter tuning and feature selection.
- Save the trained model and feature names in the `model/` directory.

## Exploratory Data Analysis (EDA)

- Use `eda_starter.py` to explore dataset statistics and visualize feature distributions between human and CG text.

## Customization

- The system is designed for general CG text detection. You can adapt the feature extraction or retrain the model for other domains (e.g., news, essays, social media).
- The category mapping in `webapp/utils/constants.py` is minimal by default; expand as needed for your data.

## Requirements

See `requirements.txt` for all dependencies. Key packages:
- streamlit, scikit-learn, xgboost, spacy, nltk, torch, transformers, datasets, seaborn

## License

This project is for educational use. Adapt and extend for your own research or applications.
